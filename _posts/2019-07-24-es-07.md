---
layout: post
title:  Elasticsearch学习笔记(07) - Analysis & Analyzer
date:   2019-07-24 22:39:30 +0800
categories: Elasticsearch
tag: Elasticsearch
---

* content
{:toc}

## 概念

+ Analysis - 文本分析，将全文本转换为一系列单词（term / token）的过程，也叫分词。
+ Analysis - 分析器，是通过 Analyzer 来实现的。在Elasticsearch中，我们可以使用其内置的分析器，也可以按需定制分析器。
+ 分词器在写入和查询时都需要用到。

## 分析器的组成

分析器由三部分组成。

1. Character Filters - 字符过滤器，针对原始文本进行处理，如去除html。
2. Tokenizer - 分词器，按照规则切分为单词，比如空格。
3. Token Filter - 特征过滤器，将切分的单词进行加工。例如：转小写、删除stopwords，增加同义词等。

例如，`Mastering Elasticsearch` & `Elasticsearch in Action`，在经过分析器之后，可能会被转换为`master / elasticsearch / action`。

## 内置分析器

在Elasticsearch中，有很多内置的分析器，其功能相对简单且强大。在介绍各个内置的分析器之前，我们有必要说明一些常用的API。

```
GET /_analyze -- 指定 Analyzer 进行测试
{
    "analyzer": "standard",
    "text": "Mastering Elasticsearch, Elasticsearch in Action"
}

POST books/_analyze -- 指定索引的字段进行测试
{
    "field": "title",
    "text": "Mastering Elasticsearch"
}

POST /_analyze -- 自定义分词进行测试
{
    "tokenizer": "standard",
    "filter": ["lowercase"],
    "text": "Mastering Elasticsearch"
}
```

| 分析器 | 作用 | 参考链接 |
| --- | --- | --- |
| Standard Analyzer | 转小写，非stop | [参考地址](https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-standard-analyzer.html) |
| Simple analyzer | 按非字母切分，非字母去除，转小写 | [参考地址](https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-simple-analyzer.html) |
| Whitespace analyzer | 按空格切分 | [参考地址](https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-whitespace-analyzer.html) |
| Stop Analyzer | 转小写，多了stop filter，如the、a、is等修饰词会被去除 | [参考地址](https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-stop-analyzer.html) |
| Keyword Analyzer | 输入直接当做输出 | [参考地址](https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-keyword-analyzer.html) |
| Pattern Analyzer | 正则表达式进行分词，默认为\W+，非字母符号进行切分 | [参考地址](https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-pattern-analyzer.html) |
| Language Analyzer | 如english | [参考地址](https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-lang-analyzer.html) |

## 中文分词

中文分词有一些难处。

1. 中文句子，需要切分成一个个词
2. 在英文中，单词有自然的空格和句点作为分隔
3. 一句中文，在不同的上下文，有不同的理解。例如：`这个苹果，不大好吃` / `这个苹果，不大，好吃！`，他们往往是两种完全不同的含义。
4. 其他的例子。他说的确实在理 / 这事的确定不下来

而Elasticsearch提供的默认分析器，对中文分词支持得不够友好。如果文本为中文，默认分析器将会按照一个一个的字进行分词，而这往往达不到我们的预期（按词拆分）。

## 中文分析器

业界比较好的中文分析器包括：

1. ICU Analyzer - 提供了Unicode支持，可以更好地支持亚洲语言
2. IK Analyzer - 开源的、好用的中文分析器
3. THULAC - 清华大学主导的中文分析器

这些分析器要想使用，需要安装相应的插件。

## 参考

+ https://blog.csdn.net/en_joker/article/details/78001760
+ https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis.html
+ https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-analyzers.html
